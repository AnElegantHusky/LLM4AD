"""
****************************************GNBG****************************************
Author: Danial Yazdani
Last Edited: March 09, 2024
Title: Generalized Numerical Benchmark Generator (GNBG)
--------
Description:
This Python code implements a set of 24 problem instances generated by the
Generalized Numerical Benchmark Generator (GNBG).
The GNBG parameter setting for each problem instance is stored in separate '.mat' files
for ease of access and execution. The code is designed to work directly with
these pre-configured instances. Note that the code operates independently of the GNBG
generator file and only uses the saved configurations of the instances.
--------
Reference:
D. Yazdani, M. N. Omidvar, D. Yazdani, K. Deb, and A. H. Gandomi, "GNBG: A Generalized
and Configurable Benchmark Generator for Continuous Numerical Optimization,"
arXiv prepring	arXiv:2312.07083, 2023.

AND

A. H. Gandomi, D. Yazdani, M. N. Omidvar, and K. Deb, "GNBG-Generated Test Suite for
Box-Constrained Numerical Global Optimization," arXiv preprint arXiv:2312.07034, 2023.

If you are using GNBG and this code in your work, you should cite the references provided above.
--------
License:
This program is to be used under the terms of the GNU General Public License
(http://www.gnu.org/copyleft/gpl.html).
Author: Danial Yazdani
e-mail: danial DOT yazdani AT gmail DOT com
Copyright notice: (c) 2023 Danial Yazdani
**************************************************************************
"""

import os

import math
import types

import numpy as np
import pandas as pd
from scipy.io import loadmat
import matplotlib.pyplot as plt
from scipy.optimize import differential_evolution

from lshade import *

mutate_v1 = '''
import numpy as np
from typing import Tuple, List
def crossover(self, x_mu=None, x=None, r=None):
    """
    Crossover the population individuals.

    Args:
        self: The instance of the class containing the mutation parameters and methods.
            - n_individuals: int, Number of individuals in the population.
            - ndim_problem: int, Dimension of the problem.
            - h: int, Length of historical memory.
            - p_min: int, Minimum population size, self.p_min = 2/self.n_individuals.
            - max_function_evaluations: int, Maximum number of function evaluations.
            - initial_pop_size: int, Initial population size.
            - _n_generations: int, Current number of generations.
            - m_median: np.ndarray, Median values of Cauchy distribution, shape=(self.h,).
            - rng_optimization: Random number generator for optimization, self.rng_optimization = np.random.default_rng(self.seed_optimization).
        x_mu: The mutated population of individuals, shape=(self.n_individuals, self.ndim_problem).
        x: The current population of individuals, shape=(self.n_individuals, self.ndim_problem).
        r: The indices of the selected individuals used for mutation and crossover, shape=(self.n_individuals,).

    Returns:
        x_cr: The crossover population of individuals, shape=(self.n_individuals, self.ndim_problem).
        p_cr: The crossover probabilities for each individual, shape=(self.n_individuals,).
    """
    import numpy as np
    from scipy.spatial.distance import cdist

    x_cr = np.copy(x_mu)
    p_cr = np.zeros(self.n_individuals)

    # Determine bracketing strategy
    bracket_size = max(4, self.n_individuals // 4)
    n_brackets = max(2, self.n_individuals // bracket_size)

    # Calculate ranks based on fitness and distance
    fitness_ranks = np.argsort(np.argsort(r))
    centroid = np.mean(x_mu, axis=0)
    distances = np.linalg.norm(x_mu - centroid, axis=1)
    distance_ranks = np.argsort(np.argsort(distances))

    # --- New Score Function Parameter Settings ---
    # The score function now uses an exponential decay for its weighting.
    # It starts with a high emphasis on exploration (80% distance) and non-linearly
    # shifts towards exploitation (80% fitness).
    generation_ratio = self._n_generations / (self.max_function_evaluations / self.initial_pop_size)
    # Weight starts at 0.8 (for distance) and decays towards 0.2.
    exploration_weight = 0.2 + 0.6 * np.exp(-5.0 * generation_ratio)
    # -------------------------------------------

    combined_scores = (1 - exploration_weight) * fitness_ranks + exploration_weight * distance_ranks
    sorted_indices = np.argsort(combined_scores)

    # Assign individuals to brackets using a serpentine (zigzag) pattern
    brackets = [[] for _ in range(n_brackets)]
    for i, idx in enumerate(sorted_indices):
        bracket_idx = i % n_brackets if (i // n_brackets) % 2 == 0 else n_brackets - 1 - (i % n_brackets)
        brackets[bracket_idx].append(idx)

    # Calculate momentum, which influences crossover probability
    momentum = 1.0 - np.exp(-self._n_generations / (self.max_function_evaluations / self.initial_pop_size * 0.1))

    # Perform crossover within each bracket
    for bracket_idx, bracket in enumerate(brackets):
        if len(bracket) < 2:
            continue

        bracket_fitness = r[bracket]
        # Measure how competitive the bracket is
        competitiveness = np.std(bracket_fitness) / (np.mean(bracket_fitness) + 1e-8)

        # Divide bracket into winners and losers based on fitness
        bracket_ranks = np.argsort(bracket_fitness)
        winners = [bracket[i] for i in bracket_ranks[:len(bracket)//2]]
        losers = [bracket[i] for i in bracket_ranks[len(bracket)//2:]]

        for i in range(len(bracket)):
            individual_idx = bracket[i]

            # Winners crossover with other winners, losers crossover with winners
            if individual_idx in winners:
                partner_pool = [w for w in winners if w != individual_idx]
                base_prob = 0.3 + 0.2 * (1 - momentum)
            else:
                partner_pool = winners
                base_prob = 0.5 + 0.3 * momentum

            # Select a crossover partner
            if partner_pool:
                partner = self.rng_optimization.choice(partner_pool)
            else: # Fallback if a pool is empty (e.g., only one winner)
                partner = self.rng_optimization.choice([idx for idx in bracket if idx != individual_idx])

            # Adjust crossover probability based on competitiveness and randomness
            competitiveness_factor = np.tanh(3 * competitiveness)
            p_cr[individual_idx] = base_prob * (0.8 + 0.3 * competitiveness_factor)
            p_cr[individual_idx] *= (0.7 + 0.3 * self.rng_optimization.random())
            p_cr[individual_idx] = np.clip(p_cr[individual_idx], 0.1, 0.9)

            # Apply uniform crossover
            mask = self.rng_optimization.random(self.ndim_problem) < p_cr[individual_idx]
            if not np.any(mask): # Ensure at least one gene is swapped
                mask[self.rng_optimization.integers(0, self.ndim_problem)] = True

            x_cr[individual_idx][mask] = x[partner][mask]

    return x_cr, p_cr'''

# Define the GNBG class
class GNBG:
    def __init__(self, MaxEvals, AcceptanceThreshold, Dimension, CompNum, MinCoordinate, MaxCoordinate, CompMinPos, CompSigma, CompH, Mu, Omega, Lambda, RotationMatrix, OptimumValue, OptimumPosition):
        self.MaxEvals = MaxEvals
        self.AcceptanceThreshold = AcceptanceThreshold
        self.Dimension = Dimension
        self.CompNum = CompNum
        self.MinCoordinate = MinCoordinate
        self.MaxCoordinate = MaxCoordinate
        self.CompMinPos = CompMinPos
        self.CompSigma = CompSigma
        self.CompH = CompH
        self.Mu = Mu
        self.Omega = Omega
        self.Lambda = Lambda
        self.RotationMatrix = RotationMatrix
        self.OptimumValue = OptimumValue
        self.OptimumPosition = OptimumPosition
        self.FEhistory = []
        self.Best_x = []
        self.FE = 0
        self.AcceptanceReachPoint = np.inf
        self.BestFoundResult = np.inf

        #DISH
        self.bounds = np.array([[self.MinCoordinate, self.MaxCoordinate] for e in range(self.Dimension)])

    def evaluate(self, X):
        if len(X.shape)<2:
            X = X.reshape(1,-1)
        SolutionNumber = X.shape[0]
        result = np.nan * np.ones(SolutionNumber)
        for jj in range(SolutionNumber):
            x = X[jj, :].reshape(-1, 1)  # Ensure column vector
            f = np.nan * np.ones(self.CompNum)
            for k in range(self.CompNum):
                if len(self.RotationMatrix.shape) == 3:
                    rotation_matrix = self.RotationMatrix[:, :, k]
                else:
                    rotation_matrix = self.RotationMatrix

                a = self.transform((x - self.CompMinPos[k, :].reshape(-1, 1)).T @ rotation_matrix.T, self.Mu[k, :], self.Omega[k, :])
                b = self.transform(rotation_matrix @ (x - self.CompMinPos[k, :].reshape(-1, 1)), self.Mu[k, :], self.Omega[k, :])
                f[k] = self.CompSigma[k] + (a @ np.diag(self.CompH[k, :]) @ b) ** self.Lambda[k]

            result[jj] = np.min(f)
            if self.FE > (self.MaxEvals-1):
                return result
            self.FE += 1
            self.FEhistory = np.append(self.FEhistory, result[jj])

            if self.BestFoundResult > result[jj]:
                self.BestFoundResult = result[jj]
                self.Best_x = X
            if abs(self.FEhistory[self.FE-1] - self.OptimumValue) < self.AcceptanceThreshold and np.isinf(self.AcceptanceReachPoint):
                self.AcceptanceReachPoint = self.FE
        return result

    def transform(self, X, Alpha, Beta):
        Y = X.copy()
        tmp = (X > 0)
        Y[tmp] = np.log(X[tmp])
        Y[tmp] = np.exp(Y[tmp] + Alpha[0] * (np.sin(Beta[0] * Y[tmp]) + np.sin(Beta[1] * Y[tmp])))
        tmp = (X < 0)
        Y[tmp] = np.log(-X[tmp])
        Y[tmp] = -np.exp(Y[tmp] + Alpha[1] * (np.sin(Beta[2] * Y[tmp]) + np.sin(Beta[3] * Y[tmp])))
        return Y


# Get the current script's directory
current_dir = os.path.dirname(os.path.abspath(__file__))

# Define the path to the folder where you want to read/write files
folder_path = os.path.join(current_dir)


def run_single(problem_index, run_index, rand_seed):
    result_path = f'results/problem_{problem_index}/'
    if os.path.exists(os.path.join(result_path, f"Problem_{problem_index}_Best_Params_Run_{run_index}.txt")):
        with open(os.path.join(result_path, f"Problem_{problem_index}_Best_Params_Run_{run_index}.txt"), 'r') as f:
            best_params = f.read()
            if best_params != "" and best_params != '[]' and False:
                print(f"Problem {problem_index}, Run {run_index} already completed. Skipping.")

                # 为每个run创建单独的文件
                if not os.path.exists(os.path.join(result_path, f"Problem_{problem_index}_Run_{run_index}.xlsx")):
                    individual_path = os.path.join(result_path, f"Problem_{problem_index}_Run_{run_index}.xlsx")

                    with open(os.path.join(result_path, f"Problem_{problem_index}_Best_Value_Run_{run_index}.txt"),
                              'r') as f:
                        best_value = float(f.read())

                    df = pd.DataFrame({
                        'Run': [run_index],
                        'BestFoundResult': [best_value],
                        'AcceptanceReachPoint': [None]
                    })

                    df.to_excel(individual_path, index=False)
                return

    print(f"Running Problem {problem_index}, Run {run_index}")
    # Define the path to the folder where you want to read/write files
    os.makedirs(result_path, exist_ok=True)

    np.random.seed()  # This uses a system-based source to seed the random number generator

    # Initialization
    ProblemIndex = problem_index  # Choose a problem instance range from f1 to f24

    # Preparation and loading of the GNBG parameters based on the chosen problem instance
    if 1 <= ProblemIndex <= 24:
        filename = os.path.join('GNBG', f'f{ProblemIndex}.mat')
        GNBG_tmp = loadmat(os.path.join(folder_path, filename))['GNBG']
        MaxEvals = np.array([item[0] for item in GNBG_tmp['MaxEvals'].flatten()])[0, 0]
        AcceptanceThreshold = np.array([item[0] for item in GNBG_tmp['AcceptanceThreshold'].flatten()])[0, 0]
        Dimension = np.array([item[0] for item in GNBG_tmp['Dimension'].flatten()])[0, 0]
        CompNum = np.array([item[0] for item in GNBG_tmp['o'].flatten()])[0, 0]  # Number of components
        MinCoordinate = np.array([item[0] for item in GNBG_tmp['MinCoordinate'].flatten()])[0, 0]
        MaxCoordinate = np.array([item[0] for item in GNBG_tmp['MaxCoordinate'].flatten()])[0, 0]
        CompMinPos = np.array(GNBG_tmp['Component_MinimumPosition'][0, 0])
        CompSigma = np.array(GNBG_tmp['ComponentSigma'][0, 0], dtype=np.float64)
        CompH = np.array(GNBG_tmp['Component_H'][0, 0])
        Mu = np.array(GNBG_tmp['Mu'][0, 0])
        Omega = np.array(GNBG_tmp['Omega'][0, 0])
        Lambda = np.array(GNBG_tmp['lambda'][0, 0])
        RotationMatrix = np.array(GNBG_tmp['RotationMatrix'][0, 0])
        OptimumValue = np.array([item[0] for item in GNBG_tmp['OptimumValue'].flatten()])[0, 0]
        OptimumPosition = np.array(GNBG_tmp['OptimumPosition'][0, 0])
    else:
        raise ValueError('ProblemIndex must be between 1 and 24.')

    gnbg = GNBG(MaxEvals, AcceptanceThreshold, Dimension, CompNum, MinCoordinate, MaxCoordinate, CompMinPos, CompSigma, CompH, Mu, Omega, Lambda, RotationMatrix, OptimumValue, OptimumPosition)

    problem = {
        'fitness_function': gnbg.evaluate,
        'ndim_problem': Dimension,
        'upper_boundary': MaxCoordinate,
        'lower_boundary': MinCoordinate,
        'initial_upper_boundary': MaxCoordinate,
        'initial_lower_boundary': MinCoordinate,
        'problem_name': f'Problem_{problem_index}',
    }
    options = {
        'max_function_evaluations': MaxEvals,
    }

    de = LSHADE(problem, options, rand_seed)
    all_globals_namespace = {}
    exec(mutate_v1, all_globals_namespace)
    program_callable = all_globals_namespace['crossover']
    de.crossover = types.MethodType(program_callable, de)
    history = de.optimize()
    print(history)

    convergence = []
    best_error = float('inf')
    for value in gnbg.FEhistory:
        error = abs(value - OptimumValue)
        if error < best_error:
            best_error = error
        convergence.append(best_error)

    # Plotting the convergence
    plt.plot(range(1, len(convergence) + 1), convergence, label=f'Problem{problem_index}_Run{run_index}')
    plt.legend()
    plt.xlabel('Function Evaluation Number (FE)')
    plt.ylabel('Error')
    plt.title('Convergence Plot')
    plt.yscale('log')  # Set y-axis to logarithmic scale
    plt.savefig(result_path + f"Problem_{problem_index}" + f"_Convergence_Run_{run_index}.pdf")

    # record convergence to txt file
    with open(result_path + f"Problem_{problem_index}" + f"_Convergence_Run_{run_index}.txt", 'w') as f:
        f.write(str(convergence))
    with open(result_path + f"Problem_{problem_index}" + f"_Best_Params_Run_{run_index}.txt", 'w') as f:
        f.write(str(gnbg.Best_x))
    with open(result_path + f"Problem_{problem_index}" + f"_Best_Value_Run_{run_index}.txt", 'w') as f:
        f.write(str(gnbg.BestFoundResult))

    # record total 31 runs of BestFoundResult and AcceptanceReachPoint to excel
    save_individual_run(run_index, gnbg, result_path, problem_index)
    print(f"Results for Problem {problem_index}, Run {run_index} finished.")

    return abs(gnbg.BestFoundResult - OptimumValue)


def append_df_to_excel(excel_path, df):
    try:
        if os.path.exists(excel_path):
            # 读取现有文件
            existing_df = pd.read_excel(excel_path)
            # 合并数据
            combined_df = pd.concat([existing_df, df], ignore_index=True)
            # 保存回文件
            combined_df.to_excel(excel_path, index=False)
        else:
            # 如果文件不存在，直接写入
            df.to_excel(excel_path, index=False)
    except Exception as e:
        print(f"Error writing to Excel: {e}")


def save_individual_run(run_index, gnbg, result_path, problem_index):
    # 为每个run创建单独的文件
    individual_path = os.path.join(result_path, f"Problem_{problem_index}_Run_{run_index}.xlsx")

    df = pd.DataFrame({
        'Run': [run_index],
        'BestFoundResult': [gnbg.BestFoundResult],
        'AcceptanceReachPoint': [gnbg.AcceptanceReachPoint]
    })

    df.to_excel(individual_path, index=False)


def merge_all_runs(result_path, problem_index, total_runs=31):
    # 合并所有运行结果
    all_dfs = []
    for run in range(total_runs):
        file_path = os.path.join(result_path, f"Problem_{problem_index}_Run_{run}.xlsx")
        if os.path.exists(file_path):
            df = pd.read_excel(file_path)
            all_dfs.append(df)
            # 可选：删除单独的文件
            os.remove(file_path)

    if all_dfs:
        final_df = pd.concat(all_dfs, ignore_index=True)
        final_path = os.path.join(result_path, f"Problem_{problem_index}_Results.xlsx")
        final_df.to_excel(final_path, index=False)

def main():
    # parallel run 31 runs for run single for 24 problems each
    import multiprocessing
    num_processes = 31  # Get the number of available CPU cores
    total_runs = 31
    total_problems = [13]
    random_seed = [2025 + i for i in range(total_runs)]
    with multiprocessing.Pool(processes=num_processes) as pool:
        results = pool.starmap(run_single, [(problem_index, run_index, random_seed[run_index]) for run_index in range(total_runs) for problem_index in total_problems])  # ,1,2,3,4,5,6,7,8,9,10,11,12,14,15,20,21,22,23,24
    # merge all runs results
    for problem_index in total_problems:
        result_path = f'results/problem_{problem_index}/'
        merge_all_runs(result_path, problem_index, total_runs)
        print(f"Results for Problem {problem_index} merged.")

    print(f'Mean Results: {results}, {np.mean(results)}')

    print("All runs completed and results merged.")


if __name__ == "__main__":

    main()
    # run_single(1, 0)


