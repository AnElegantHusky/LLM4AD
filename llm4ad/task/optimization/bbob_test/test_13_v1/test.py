"""
****************************************GNBG****************************************
Author: Danial Yazdani
Last Edited: March 09, 2024
Title: Generalized Numerical Benchmark Generator (GNBG)
--------
Description:
This Python code implements a set of 24 problem instances generated by the
Generalized Numerical Benchmark Generator (GNBG).
The GNBG parameter setting for each problem instance is stored in separate '.mat' files
for ease of access and execution. The code is designed to work directly with
these pre-configured instances. Note that the code operates independently of the GNBG
generator file and only uses the saved configurations of the instances.
--------
Reference:
D. Yazdani, M. N. Omidvar, D. Yazdani, K. Deb, and A. H. Gandomi, "GNBG: A Generalized
and Configurable Benchmark Generator for Continuous Numerical Optimization,"
arXiv prepring	arXiv:2312.07083, 2023.

AND

A. H. Gandomi, D. Yazdani, M. N. Omidvar, and K. Deb, "GNBG-Generated Test Suite for
Box-Constrained Numerical Global Optimization," arXiv preprint arXiv:2312.07034, 2023.

If you are using GNBG and this code in your work, you should cite the references provided above.
--------
License:
This program is to be used under the terms of the GNU General Public License
(http://www.gnu.org/copyleft/gpl.html).
Author: Danial Yazdani
e-mail: danial DOT yazdani AT gmail DOT com
Copyright notice: (c) 2023 Danial Yazdani
**************************************************************************
"""

import os

import math
import types

import numpy as np
import pandas as pd
from scipy.io import loadmat
import matplotlib.pyplot as plt
from scipy.optimize import differential_evolution

from lshade import *

mutate_v1 = '''
import numpy as np
from typing import Tuple, List
def crossover(self, x_mu=None, x=None, r=None):
    """
    Crossover the population individuals.

    Args:
        self: The instance of the class containing the mutation parameters and methods.
            - n_individuals: int, Number of individuals in the population.
            - ndim_problem: int, Dimension of the problem.
            - h: int, Length of historical memory.
            - p_min: int, Minimum population size, self.p_min = 2/self.n_individuals.
            - max_function_evaluations: int, Maximum number of function evaluations.
            - initial_pop_size: int, Initial population size.
            - _n_generations: int, Current number of generations.
            - m_median: np.ndarray, Median values of Cauchy distribution, shape=(self.h,).
            - rng_optimization: Random number generator for optimization, self.rng_optimization = np.random.default_rng(self.seed_optimization).
        x_mu: The mutated population of individuals, shape=(self.n_individuals, self.ndim_problem).
        x: The current population of individuals, shape=(self.n_individuals, self.ndim_problem).
        r: The indices of the selected individuals used for mutation and crossover, shape=(self.n_individuals,).

    Returns:
        x_cr: The crossover population of individuals, shape=(self.n_individuals, self.ndim_problem).
        p_cr: The crossover probabilities for each individual, shape=(self.n_individuals,).
    """
    import numpy as np

    x_cr = np.copy(x_mu)
    p_cr = np.zeros(self.n_individuals)

    # Dynamic cluster adaptation based on moving average threshold
    mavg_window = min(5, self._n_generations + 1)
    dynamic_threshold = np.mean([np.linalg.norm(x_mu - np.mean(x_mu, axis=0), axis=1)[-mavg_window:]])

    clusters = np.array([np.linalg.norm(x_mu[i] - np.mean(x_mu, axis=0)) < dynamic_threshold for i in range(self.n_individuals)], dtype=int)

    # Variable-probability neighbor exploration
    variable_radius = np.std(x_mu) * (0.5 + 0.5 * np.sin(self._n_generations))

    for i in range(self.n_individuals):
        # Identify neighbors using variable radius
        distances = np.linalg.norm(x_mu - x_mu[i], axis=1)
        neighbor_indices = np.where(distances <= variable_radius)[0]

        if len(neighbor_indices) < 2:
            neighbor_indices = np.arange(self.n_individuals)

        # Select partner based on combined fitness and dynamic cluster transitions
        partner_candidates = neighbor_indices[clusters[neighbor_indices] == clusters[i]]
        partner = self.rng_optimization.choice(partner_candidates)

        # Recombine with variable dimensional probabilities
        variable_dim_probs = 0.3 + 0.4 * np.abs(np.cos(self._n_generations))
        p_cr[i] = np.mean(variable_dim_probs)

        # Dimensional crossover
        mask = self.rng_optimization.random(self.ndim_problem) < variable_dim_probs
        if not np.any(mask):
            mask[self.rng_optimization.integers(0, self.ndim_problem)] = True
        x_cr[i][mask] = x[partner][mask]

    return x_cr, p_cr'''

# Define the GNBG class
class GNBG:
    def __init__(self, MaxEvals, AcceptanceThreshold, Dimension, CompNum, MinCoordinate, MaxCoordinate, CompMinPos, CompSigma, CompH, Mu, Omega, Lambda, RotationMatrix, OptimumValue, OptimumPosition):
        self.MaxEvals = MaxEvals
        self.AcceptanceThreshold = AcceptanceThreshold
        self.Dimension = Dimension
        self.CompNum = CompNum
        self.MinCoordinate = MinCoordinate
        self.MaxCoordinate = MaxCoordinate
        self.CompMinPos = CompMinPos
        self.CompSigma = CompSigma
        self.CompH = CompH
        self.Mu = Mu
        self.Omega = Omega
        self.Lambda = Lambda
        self.RotationMatrix = RotationMatrix
        self.OptimumValue = OptimumValue
        self.OptimumPosition = OptimumPosition
        self.FEhistory = []
        self.Best_x = []
        self.FE = 0
        self.AcceptanceReachPoint = np.inf
        self.BestFoundResult = np.inf

        #DISH
        self.bounds = np.array([[self.MinCoordinate, self.MaxCoordinate] for e in range(self.Dimension)])

    def evaluate(self, X):
        if len(X.shape)<2:
            X = X.reshape(1,-1)
        SolutionNumber = X.shape[0]
        result = np.nan * np.ones(SolutionNumber)
        for jj in range(SolutionNumber):
            x = X[jj, :].reshape(-1, 1)  # Ensure column vector
            f = np.nan * np.ones(self.CompNum)
            for k in range(self.CompNum):
                if len(self.RotationMatrix.shape) == 3:
                    rotation_matrix = self.RotationMatrix[:, :, k]
                else:
                    rotation_matrix = self.RotationMatrix

                a = self.transform((x - self.CompMinPos[k, :].reshape(-1, 1)).T @ rotation_matrix.T, self.Mu[k, :], self.Omega[k, :])
                b = self.transform(rotation_matrix @ (x - self.CompMinPos[k, :].reshape(-1, 1)), self.Mu[k, :], self.Omega[k, :])
                f[k] = self.CompSigma[k] + (a @ np.diag(self.CompH[k, :]) @ b) ** self.Lambda[k]

            result[jj] = np.min(f)
            if self.FE > (self.MaxEvals-1):
                return result
            self.FE += 1
            self.FEhistory = np.append(self.FEhistory, result[jj])

            if self.BestFoundResult > result[jj]:
                self.BestFoundResult = result[jj]
                self.Best_x = X
            if abs(self.FEhistory[self.FE-1] - self.OptimumValue) < self.AcceptanceThreshold and np.isinf(self.AcceptanceReachPoint):
                self.AcceptanceReachPoint = self.FE
        return result

    def transform(self, X, Alpha, Beta):
        Y = X.copy()
        tmp = (X > 0)
        Y[tmp] = np.log(X[tmp])
        Y[tmp] = np.exp(Y[tmp] + Alpha[0] * (np.sin(Beta[0] * Y[tmp]) + np.sin(Beta[1] * Y[tmp])))
        tmp = (X < 0)
        Y[tmp] = np.log(-X[tmp])
        Y[tmp] = -np.exp(Y[tmp] + Alpha[1] * (np.sin(Beta[2] * Y[tmp]) + np.sin(Beta[3] * Y[tmp])))
        return Y


# Get the current script's directory
current_dir = os.path.dirname(os.path.abspath(__file__))

# Define the path to the folder where you want to read/write files
folder_path = os.path.join(current_dir)


def run_single(problem_index, run_index, rand_seed):
    result_path = f'results/problem_{problem_index}/'
    if os.path.exists(os.path.join(result_path, f"Problem_{problem_index}_Best_Params_Run_{run_index}.txt")):
        with open(os.path.join(result_path, f"Problem_{problem_index}_Best_Params_Run_{run_index}.txt"), 'r') as f:
            best_params = f.read()
            if best_params != "" and best_params != '[]' and False:
                print(f"Problem {problem_index}, Run {run_index} already completed. Skipping.")

                # 为每个run创建单独的文件
                if not os.path.exists(os.path.join(result_path, f"Problem_{problem_index}_Run_{run_index}.xlsx")):
                    individual_path = os.path.join(result_path, f"Problem_{problem_index}_Run_{run_index}.xlsx")

                    with open(os.path.join(result_path, f"Problem_{problem_index}_Best_Value_Run_{run_index}.txt"),
                              'r') as f:
                        best_value = float(f.read())

                    df = pd.DataFrame({
                        'Run': [run_index],
                        'BestFoundResult': [best_value],
                        'AcceptanceReachPoint': [None]
                    })

                    df.to_excel(individual_path, index=False)
                return

    print(f"Running Problem {problem_index}, Run {run_index}")
    # Define the path to the folder where you want to read/write files
    os.makedirs(result_path, exist_ok=True)

    np.random.seed()  # This uses a system-based source to seed the random number generator

    # Initialization
    ProblemIndex = problem_index  # Choose a problem instance range from f1 to f24

    # Preparation and loading of the GNBG parameters based on the chosen problem instance
    if 1 <= ProblemIndex <= 24:
        filename = os.path.join('GNBG', f'f{ProblemIndex}.mat')
        GNBG_tmp = loadmat(os.path.join(folder_path, filename))['GNBG']
        MaxEvals = np.array([item[0] for item in GNBG_tmp['MaxEvals'].flatten()])[0, 0]
        AcceptanceThreshold = np.array([item[0] for item in GNBG_tmp['AcceptanceThreshold'].flatten()])[0, 0]
        Dimension = np.array([item[0] for item in GNBG_tmp['Dimension'].flatten()])[0, 0]
        CompNum = np.array([item[0] for item in GNBG_tmp['o'].flatten()])[0, 0]  # Number of components
        MinCoordinate = np.array([item[0] for item in GNBG_tmp['MinCoordinate'].flatten()])[0, 0]
        MaxCoordinate = np.array([item[0] for item in GNBG_tmp['MaxCoordinate'].flatten()])[0, 0]
        CompMinPos = np.array(GNBG_tmp['Component_MinimumPosition'][0, 0])
        CompSigma = np.array(GNBG_tmp['ComponentSigma'][0, 0], dtype=np.float64)
        CompH = np.array(GNBG_tmp['Component_H'][0, 0])
        Mu = np.array(GNBG_tmp['Mu'][0, 0])
        Omega = np.array(GNBG_tmp['Omega'][0, 0])
        Lambda = np.array(GNBG_tmp['lambda'][0, 0])
        RotationMatrix = np.array(GNBG_tmp['RotationMatrix'][0, 0])
        OptimumValue = np.array([item[0] for item in GNBG_tmp['OptimumValue'].flatten()])[0, 0]
        OptimumPosition = np.array(GNBG_tmp['OptimumPosition'][0, 0])
    else:
        raise ValueError('ProblemIndex must be between 1 and 24.')

    gnbg = GNBG(MaxEvals, AcceptanceThreshold, Dimension, CompNum, MinCoordinate, MaxCoordinate, CompMinPos, CompSigma, CompH, Mu, Omega, Lambda, RotationMatrix, OptimumValue, OptimumPosition)

    problem = {
        'fitness_function': gnbg.evaluate,
        'ndim_problem': Dimension,
        'upper_boundary': MaxCoordinate,
        'lower_boundary': MinCoordinate,
        'initial_upper_boundary': MaxCoordinate,
        'initial_lower_boundary': MinCoordinate,
        'problem_name': f'Problem_{problem_index}',
    }
    options = {
        'max_function_evaluations': MaxEvals,
    }

    de = LSHADE(problem, options, rand_seed)
    all_globals_namespace = {}
    exec(mutate_v1, all_globals_namespace)
    program_callable = all_globals_namespace['crossover']
    de.crossover = types.MethodType(program_callable, de)
    history = de.optimize()
    print(history)

    convergence = []
    best_error = float('inf')
    for value in gnbg.FEhistory:
        error = abs(value - OptimumValue)
        if error < best_error:
            best_error = error
        convergence.append(best_error)

    # Plotting the convergence
    plt.plot(range(1, len(convergence) + 1), convergence, label=f'Problem{problem_index}_Run{run_index}')
    plt.legend()
    plt.xlabel('Function Evaluation Number (FE)')
    plt.ylabel('Error')
    plt.title('Convergence Plot')
    plt.yscale('log')  # Set y-axis to logarithmic scale
    plt.savefig(result_path + f"Problem_{problem_index}" + f"_Convergence_Run_{run_index}.pdf")

    # record convergence to txt file
    with open(result_path + f"Problem_{problem_index}" + f"_Convergence_Run_{run_index}.txt", 'w') as f:
        f.write(str(convergence))
    with open(result_path + f"Problem_{problem_index}" + f"_Best_Params_Run_{run_index}.txt", 'w') as f:
        f.write(str(gnbg.Best_x))
    with open(result_path + f"Problem_{problem_index}" + f"_Best_Value_Run_{run_index}.txt", 'w') as f:
        f.write(str(gnbg.BestFoundResult))

    # record total 31 runs of BestFoundResult and AcceptanceReachPoint to excel
    save_individual_run(run_index, gnbg, result_path, problem_index)
    print(f"Results for Problem {problem_index}, Run {run_index} finished.")

    return abs(gnbg.BestFoundResult - OptimumValue)


def append_df_to_excel(excel_path, df):
    try:
        if os.path.exists(excel_path):
            # 读取现有文件
            existing_df = pd.read_excel(excel_path)
            # 合并数据
            combined_df = pd.concat([existing_df, df], ignore_index=True)
            # 保存回文件
            combined_df.to_excel(excel_path, index=False)
        else:
            # 如果文件不存在，直接写入
            df.to_excel(excel_path, index=False)
    except Exception as e:
        print(f"Error writing to Excel: {e}")


def save_individual_run(run_index, gnbg, result_path, problem_index):
    # 为每个run创建单独的文件
    individual_path = os.path.join(result_path, f"Problem_{problem_index}_Run_{run_index}.xlsx")

    df = pd.DataFrame({
        'Run': [run_index],
        'BestFoundResult': [gnbg.BestFoundResult],
        'AcceptanceReachPoint': [gnbg.AcceptanceReachPoint]
    })

    df.to_excel(individual_path, index=False)


def merge_all_runs(result_path, problem_index, total_runs=31):
    # 合并所有运行结果
    all_dfs = []
    for run in range(total_runs):
        file_path = os.path.join(result_path, f"Problem_{problem_index}_Run_{run}.xlsx")
        if os.path.exists(file_path):
            df = pd.read_excel(file_path)
            all_dfs.append(df)
            # 可选：删除单独的文件
            os.remove(file_path)

    if all_dfs:
        final_df = pd.concat(all_dfs, ignore_index=True)
        final_path = os.path.join(result_path, f"Problem_{problem_index}_Results.xlsx")
        final_df.to_excel(final_path, index=False)

def main():
    # parallel run 31 runs for run single for 24 problems each
    import multiprocessing
    num_processes = 31  # Get the number of available CPU cores
    total_runs = 31
    total_problems = [13]
    random_seed = [2025 + i for i in range(total_runs)]
    with multiprocessing.Pool(processes=num_processes) as pool:
        results = pool.starmap(run_single, [(problem_index, run_index, random_seed[run_index]) for run_index in range(total_runs) for problem_index in total_problems])  # ,1,2,3,4,5,6,7,8,9,10,11,12,14,15,20,21,22,23,24
    # merge all runs results
    for problem_index in total_problems:
        result_path = f'results/problem_{problem_index}/'
        merge_all_runs(result_path, problem_index, total_runs)
        print(f"Results for Problem {problem_index} merged.")

    print(f'Mean Results: {results}, {np.mean(results)}')

    print("All runs completed and results merged.")


if __name__ == "__main__":

    main()
    # run_single(1, 0)


