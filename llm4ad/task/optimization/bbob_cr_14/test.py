"""
****************************************GNBG****************************************
Author: Danial Yazdani
Last Edited: March 09, 2024
Title: Generalized Numerical Benchmark Generator (GNBG)
--------
Description:
This Python code implements a set of 24 problem instances generated by the
Generalized Numerical Benchmark Generator (GNBG).
The GNBG parameter setting for each problem instance is stored in separate '.mat' files
for ease of access and execution. The code is designed to work directly with
these pre-configured instances. Note that the code operates independently of the GNBG
generator file and only uses the saved configurations of the instances.
--------
Reference:
D. Yazdani, M. N. Omidvar, D. Yazdani, K. Deb, and A. H. Gandomi, "GNBG: A Generalized
and Configurable Benchmark Generator for Continuous Numerical Optimization,"
arXiv prepring	arXiv:2312.07083, 2023.

AND

A. H. Gandomi, D. Yazdani, M. N. Omidvar, and K. Deb, "GNBG-Generated Test Suite for
Box-Constrained Numerical Global Optimization," arXiv preprint arXiv:2312.07034, 2023.

If you are using GNBG and this code in your work, you should cite the references provided above.
--------
License:
This program is to be used under the terms of the GNU General Public License
(http://www.gnu.org/copyleft/gpl.html).
Author: Danial Yazdani
e-mail: danial DOT yazdani AT gmail DOT com
Copyright notice: (c) 2023 Danial Yazdani
**************************************************************************
"""

import os

import math
import types

import numpy as np
import pandas as pd
from scipy.io import loadmat
import matplotlib.pyplot as plt
from scipy.optimize import differential_evolution

from lshade import *

mutate_v1 = '''
import numpy as np
from typing import Tuple, List
def initialize(self, args=None):
    """
    Crossover the population individuals.

    Args:
        self: The instance of the class containing the mutation parameters and methods.
            - n_individuals: int, Number of individuals in the population.
            - ndim_problem: int, Dimension of the problem.
            - initial_lower_boundary: int, Lower boundary for the initial population.
            - initial_upper_boundary: int, Upper boundary for the initial population.
            - _check_terminations(): method, 
                    Method to check termination conditions, 
                    Return True if reach the terminations else False.
            - h: int, Length of historical memory.
            - max_function_evaluations: int, Maximum number of function evaluations.
            - initial_pop_size: int, Initial population size.
            - _n_generations: int, Current number of generations.
            - m_median: np.ndarray, Median values of Cauchy distribution, shape=(self.h,).
            - _evaluate_fitness(individual: np.ndarray, args): method, 
                    Method to evaluate the fitness of an individual, input individual shape=(self.ndim_problem,).
                    Return fitness value of the individual.
            - rng_optimization: Random number generator for optimization, self.rng_optimization = np.random.default_rng(self.seed_optimization).

    Returns:
        x: np.ndarray, The current population of individuals, shape=(self.n_individuals, self.ndim_problem).
        y: np.ndarray, The current fitness values of the population, shape=(self.n_individuals,).
        a: np.ndarray, The archive of inferior solutions, shape=(0, self.ndim_problem).
    """
    x = np.zeros((self.n_individuals, self.ndim_problem))
    
    # Generate spiral parameters
    n_spirals = max(2, int(np.sqrt(self.ndim_problem)))
    spiral_density = self.n_individuals // n_spirals
    remaining_individuals = self.n_individuals % n_spirals
    
    # Create spiral centers using prime number spacing
    primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]
    spiral_centers = np.zeros((n_spirals, self.ndim_problem))
    
    for s in range(n_spirals):
        for d in range(self.ndim_problem):
            prime_idx = (s + d) % len(primes)
            center_offset = (primes[prime_idx] * (s + 1)) % 100 / 100.0
            spiral_centers[s, d] = self.initial_lower_boundary + center_offset * (
                self.initial_upper_boundary - self.initial_lower_boundary
            )
    
    current_idx = 0
    domain_range = self.initial_upper_boundary - self.initial_lower_boundary
    max_radius = 0.6 * np.linalg.norm(domain_range)
    
    # Generate individuals along spirals
    for spiral_id in range(n_spirals):
        current_spiral_size = spiral_density + (1 if spiral_id < remaining_individuals else 0)
        spiral_center = spiral_centers[spiral_id]
        
        for i in range(current_spiral_size):
            # Calculate spiral position parameters
            progress = i / max(1, current_spiral_size - 1)
            
            # Dynamic radius with exponential decay
            radius = max_radius * (1 - progress) ** 1.5
            
            # Spiral angle with golden ratio increment
            golden_ratio = 1.618033988749
            angle_increment = 2 * np.pi / golden_ratio
            spiral_angle = i * angle_increment + spiral_id * np.pi / 3
            
            # Generate position using polar coordinates
            position_offset = np.zeros(self.ndim_problem)
            for d in range(self.ndim_problem):
                dim_angle = spiral_angle + d * 2 * np.pi / self.ndim_problem
                radial_component = radius * np.cos(dim_angle + spiral_id * 0.5)
                tangential_component = radius * np.sin(dim_angle + spiral_id * 0.5) * 0.7
                
                position_offset[d] = radial_component + tangential_component * (d % 2 * 2 - 1)
            
            # Apply adaptive scaling based on dimension
            dimension_scale = 1.0 / np.sqrt(self.ndim_problem)
            position_offset *= dimension_scale
            
            # Add polar noise
            polar_noise_strength = 0.15 * (1 - progress * 0.8)
            polar_noise = self.rng_optimization.vonmises(0, 2.0, self.ndim_problem)
            scaled_noise = polar_noise * polar_noise_strength * domain_range
            
            # Compute candidate position
            x[current_idx] = spiral_center + position_offset + scaled_noise
            
            # Reflective boundary handling
            for d in range(self.ndim_problem):
                if x[current_idx, d] < self.initial_lower_boundary:
                    x[current_idx, d] = 2 * self.initial_lower_boundary - x[current_idx, d]
                elif x[current_idx, d] > self.initial_upper_boundary:
                    x[current_idx, d] = 2 * self.initial_upper_boundary - x[current_idx, d]
                
                # Final clipping
                x[current_idx, d] = np.clip(x[current_idx, d], 
                                          self.initial_lower_boundary, 
                                          self.initial_upper_boundary)
            
            current_idx += 1
    
    # Competitive positioning phase
    competition_fraction = 0.3
    n_competitors = int(competition_fraction * self.n_individuals)
    competitor_indices = self.rng_optimization.choice(self.n_individuals, n_competitors, replace=False)
    
    for idx in competitor_indices:
        # Find nearest neighbors
        distances = np.array([np.linalg.norm(x[idx] - x[j]) for j in range(self.n_individuals) if j != idx])
        nearest_count = min(3, len(distances))
        nearest_indices = np.argsort(distances)[:nearest_count]
        
        # Calculate repulsion vector
        repulsion_vector = np.zeros(self.ndim_problem)
        for neighbor_idx in nearest_indices:
            if neighbor_idx >= idx:
                neighbor_idx += 1
            
            direction = x[idx] - x[neighbor_idx]
            distance = np.linalg.norm(direction)
            if distance > 0:
                repulsion_vector += direction / (distance ** 2 + 1e-8)
        
        # Apply competitive displacement
        displacement_strength = 0.2 * domain_range
        if np.linalg.norm(repulsion_vector) > 0:
            repulsion_vector = repulsion_vector / np.linalg.norm(repulsion_vector)
            x[idx] += displacement_strength * repulsion_vector * self.rng_optimization.uniform(0.5, 1.0)
        
        # Boundary enforcement
        x[idx] = np.clip(x[idx], self.initial_lower_boundary, self.initial_upper_boundary)
    
    # Evaluate fitness for all individuals
    y = np.zeros(self.n_individuals)
    for i in range(self.n_individuals):
        y[i] = self._evaluate_fitness(x[i], args)
        if self._check_terminations():
            break
    
    # Initialize empty archive
    a = np.zeros((0, self.ndim_problem))
    
    return x, y, a'''

# Define the GNBG class
class GNBG:
    def __init__(self, MaxEvals, AcceptanceThreshold, Dimension, CompNum, MinCoordinate, MaxCoordinate, CompMinPos, CompSigma, CompH, Mu, Omega, Lambda, RotationMatrix, OptimumValue, OptimumPosition):
        self.MaxEvals = MaxEvals
        self.AcceptanceThreshold = AcceptanceThreshold
        self.Dimension = Dimension
        self.CompNum = CompNum
        self.MinCoordinate = MinCoordinate
        self.MaxCoordinate = MaxCoordinate
        self.CompMinPos = CompMinPos
        self.CompSigma = CompSigma
        self.CompH = CompH
        self.Mu = Mu
        self.Omega = Omega
        self.Lambda = Lambda
        self.RotationMatrix = RotationMatrix
        self.OptimumValue = OptimumValue
        self.OptimumPosition = OptimumPosition
        self.FEhistory = []
        self.Best_x = []
        self.FE = 0
        self.AcceptanceReachPoint = np.inf
        self.BestFoundResult = np.inf

        #DISH
        self.bounds = np.array([[self.MinCoordinate, self.MaxCoordinate] for e in range(self.Dimension)])

    def evaluate(self, X):
        if len(X.shape)<2:
            X = X.reshape(1,-1)
        SolutionNumber = X.shape[0]
        result = np.nan * np.ones(SolutionNumber)
        for jj in range(SolutionNumber):
            x = X[jj, :].reshape(-1, 1)  # Ensure column vector
            f = np.nan * np.ones(self.CompNum)
            for k in range(self.CompNum):
                if len(self.RotationMatrix.shape) == 3:
                    rotation_matrix = self.RotationMatrix[:, :, k]
                else:
                    rotation_matrix = self.RotationMatrix

                a = self.transform((x - self.CompMinPos[k, :].reshape(-1, 1)).T @ rotation_matrix.T, self.Mu[k, :], self.Omega[k, :])
                b = self.transform(rotation_matrix @ (x - self.CompMinPos[k, :].reshape(-1, 1)), self.Mu[k, :], self.Omega[k, :])
                f[k] = self.CompSigma[k] + (a @ np.diag(self.CompH[k, :]) @ b) ** self.Lambda[k]

            result[jj] = np.min(f)
            if self.FE > (self.MaxEvals-1):
                return result
            self.FE += 1
            self.FEhistory = np.append(self.FEhistory, result[jj])

            if self.BestFoundResult > result[jj]:
                self.BestFoundResult = result[jj]
                self.Best_x = X
            if abs(self.FEhistory[self.FE-1] - self.OptimumValue) < self.AcceptanceThreshold and np.isinf(self.AcceptanceReachPoint):
                self.AcceptanceReachPoint = self.FE
        return result

    def transform(self, X, Alpha, Beta):
        Y = X.copy()
        tmp = (X > 0)
        Y[tmp] = np.log(X[tmp])
        Y[tmp] = np.exp(Y[tmp] + Alpha[0] * (np.sin(Beta[0] * Y[tmp]) + np.sin(Beta[1] * Y[tmp])))
        tmp = (X < 0)
        Y[tmp] = np.log(-X[tmp])
        Y[tmp] = -np.exp(Y[tmp] + Alpha[1] * (np.sin(Beta[2] * Y[tmp]) + np.sin(Beta[3] * Y[tmp])))
        return Y


# Get the current script's directory
current_dir = os.path.dirname(os.path.abspath(__file__))

# Define the path to the folder where you want to read/write files
folder_path = os.path.join(current_dir)


def run_single(problem_index, run_index, rand_seed):
    result_path = f'results/problem_{problem_index}/'
    if os.path.exists(os.path.join(result_path, f"Problem_{problem_index}_Best_Params_Run_{run_index}.txt")):
        with open(os.path.join(result_path, f"Problem_{problem_index}_Best_Params_Run_{run_index}.txt"), 'r') as f:
            best_params = f.read()
            if best_params != "" and best_params != '[]' and False:
                print(f"Problem {problem_index}, Run {run_index} already completed. Skipping.")

                # 为每个run创建单独的文件
                if not os.path.exists(os.path.join(result_path, f"Problem_{problem_index}_Run_{run_index}.xlsx")):
                    individual_path = os.path.join(result_path, f"Problem_{problem_index}_Run_{run_index}.xlsx")

                    with open(os.path.join(result_path, f"Problem_{problem_index}_Best_Value_Run_{run_index}.txt"),
                              'r') as f:
                        best_value = float(f.read())

                    df = pd.DataFrame({
                        'Run': [run_index],
                        'BestFoundResult': [best_value],
                        'AcceptanceReachPoint': [None]
                    })

                    df.to_excel(individual_path, index=False)
                return

    print(f"Running Problem {problem_index}, Run {run_index}")
    # Define the path to the folder where you want to read/write files
    os.makedirs(result_path, exist_ok=True)

    np.random.seed()  # This uses a system-based source to seed the random number generator

    # Initialization
    ProblemIndex = problem_index  # Choose a problem instance range from f1 to f24

    # Preparation and loading of the GNBG parameters based on the chosen problem instance
    if 1 <= ProblemIndex <= 24:
        filename = os.path.join('GNBG', f'f{ProblemIndex}.mat')
        GNBG_tmp = loadmat(os.path.join(folder_path, filename))['GNBG']
        MaxEvals = np.array([item[0] for item in GNBG_tmp['MaxEvals'].flatten()])[0, 0]
        AcceptanceThreshold = np.array([item[0] for item in GNBG_tmp['AcceptanceThreshold'].flatten()])[0, 0]
        Dimension = np.array([item[0] for item in GNBG_tmp['Dimension'].flatten()])[0, 0]
        CompNum = np.array([item[0] for item in GNBG_tmp['o'].flatten()])[0, 0]  # Number of components
        MinCoordinate = np.array([item[0] for item in GNBG_tmp['MinCoordinate'].flatten()])[0, 0]
        MaxCoordinate = np.array([item[0] for item in GNBG_tmp['MaxCoordinate'].flatten()])[0, 0]
        CompMinPos = np.array(GNBG_tmp['Component_MinimumPosition'][0, 0])
        CompSigma = np.array(GNBG_tmp['ComponentSigma'][0, 0], dtype=np.float64)
        CompH = np.array(GNBG_tmp['Component_H'][0, 0])
        Mu = np.array(GNBG_tmp['Mu'][0, 0])
        Omega = np.array(GNBG_tmp['Omega'][0, 0])
        Lambda = np.array(GNBG_tmp['lambda'][0, 0])
        RotationMatrix = np.array(GNBG_tmp['RotationMatrix'][0, 0])
        OptimumValue = np.array([item[0] for item in GNBG_tmp['OptimumValue'].flatten()])[0, 0]
        OptimumPosition = np.array(GNBG_tmp['OptimumPosition'][0, 0])
    else:
        raise ValueError('ProblemIndex must be between 1 and 24.')

    gnbg = GNBG(MaxEvals, AcceptanceThreshold, Dimension, CompNum, MinCoordinate, MaxCoordinate, CompMinPos, CompSigma, CompH, Mu, Omega, Lambda, RotationMatrix, OptimumValue, OptimumPosition)

    problem = {
        'fitness_function': gnbg.evaluate,
        'ndim_problem': Dimension,
        'upper_boundary': MaxCoordinate,
        'lower_boundary': MinCoordinate,
        'initial_upper_boundary': MaxCoordinate,
        'initial_lower_boundary': MinCoordinate,
        'problem_name': f'Problem_{problem_index}',
    }
    options = {
        'max_function_evaluations': MaxEvals,
    }

    de = LSHADE(problem, options, rand_seed)
    all_globals_namespace = {}
    exec(mutate_v1, all_globals_namespace)
    program_callable = all_globals_namespace['initialize']
    de.initialize = types.MethodType(program_callable, de)
    history = de.optimize()
    print(history)

    convergence = []
    best_error = float('inf')
    for value in gnbg.FEhistory:
        error = abs(value - OptimumValue)
        if error < best_error:
            best_error = error
        convergence.append(best_error)

    # Plotting the convergence
    plt.plot(range(1, len(convergence) + 1), convergence, label=f'Problem{problem_index}_Run{run_index}')
    plt.legend()
    plt.xlabel('Function Evaluation Number (FE)')
    plt.ylabel('Error')
    plt.title('Convergence Plot')
    plt.yscale('log')  # Set y-axis to logarithmic scale
    plt.savefig(result_path + f"Problem_{problem_index}" + f"_Convergence_Run_{run_index}.pdf")

    # record convergence to txt file
    with open(result_path + f"Problem_{problem_index}" + f"_Convergence_Run_{run_index}.txt", 'w') as f:
        f.write(str(convergence))
    with open(result_path + f"Problem_{problem_index}" + f"_Best_Params_Run_{run_index}.txt", 'w') as f:
        f.write(str(gnbg.Best_x))
    with open(result_path + f"Problem_{problem_index}" + f"_Best_Value_Run_{run_index}.txt", 'w') as f:
        f.write(str(gnbg.BestFoundResult))

    # record total 31 runs of BestFoundResult and AcceptanceReachPoint to excel
    save_individual_run(run_index, gnbg, result_path, problem_index)
    print(f"Results for Problem {problem_index}, Run {run_index} finished.")

    return abs(gnbg.BestFoundResult - OptimumValue)


def append_df_to_excel(excel_path, df):
    try:
        if os.path.exists(excel_path):
            # 读取现有文件
            existing_df = pd.read_excel(excel_path)
            # 合并数据
            combined_df = pd.concat([existing_df, df], ignore_index=True)
            # 保存回文件
            combined_df.to_excel(excel_path, index=False)
        else:
            # 如果文件不存在，直接写入
            df.to_excel(excel_path, index=False)
    except Exception as e:
        print(f"Error writing to Excel: {e}")


def save_individual_run(run_index, gnbg, result_path, problem_index):
    # 为每个run创建单独的文件
    individual_path = os.path.join(result_path, f"Problem_{problem_index}_Run_{run_index}.xlsx")

    df = pd.DataFrame({
        'Run': [run_index],
        'BestFoundResult': [gnbg.BestFoundResult],
        'AcceptanceReachPoint': [gnbg.AcceptanceReachPoint]
    })

    df.to_excel(individual_path, index=False)


def merge_all_runs(result_path, problem_index, total_runs=31):
    # 合并所有运行结果
    all_dfs = []
    for run in range(total_runs):
        file_path = os.path.join(result_path, f"Problem_{problem_index}_Run_{run}.xlsx")
        if os.path.exists(file_path):
            df = pd.read_excel(file_path)
            all_dfs.append(df)
            # 可选：删除单独的文件
            os.remove(file_path)

    if all_dfs:
        final_df = pd.concat(all_dfs, ignore_index=True)
        final_path = os.path.join(result_path, f"Problem_{problem_index}_Results.xlsx")
        final_df.to_excel(final_path, index=False)

def main():
    # parallel run 31 runs for run single for 24 problems each
    import multiprocessing
    num_processes = 70  # Get the number of available CPU cores
    total_runs = 31
    total_problems = [13,16,17,18,19,1,2,3,4,5,6,7,8,9,10,11,12,14,15,20,21,22,23,24]
    random_seed = [2025 + i for i in range(total_runs)]
    with multiprocessing.Pool(processes=num_processes) as pool:
        results = pool.starmap(run_single, [(problem_index, run_index, random_seed[run_index]) for run_index in range(total_runs) for problem_index in total_problems])  # ,1,2,3,4,5,6,7,8,9,10,11,12,14,15,20,21,22,23,24
    # merge all runs results
    for problem_index in total_problems:
        result_path = f'results/problem_{problem_index}/'
        merge_all_runs(result_path, problem_index, total_runs)
        print(f"Results for Problem {problem_index} merged.")

    print(f'Mean Results: {results}, {np.mean(results)}')

    print("All runs completed and results merged.")


if __name__ == "__main__":

    main()
    # run_single(1, 0)


